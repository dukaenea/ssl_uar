<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="UTF-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <meta name="generator" content="Jekyll v3.9.0">
    <meta property="og:title" content="[&quot;Leveraging Self-Supervised Training for Unintentional Action
    Recognition&quot;]">
    <meta property="og:locale" content="en_US">
    <meta name="description" content="[&quot;ECCVW 2022 (SSLWIN)&quot;]">
    <meta property="og:description" content="[&quot;ECCVW 2022 (SSLWIN)&quot;]">
    <link rel="canonical" href="https://annusha.github.io/LIReC/">
    <meta property="og:url" content="https://annusha.github.io/LIReC/">
    <meta property="og:site_name" content="[&quot;Learning Interactions and Relationships between Movie Characters&quot;]">
    <script type="application/ld+json">
    {"@type":"WebSite","url":"https://annusha.github.io/LIReC/","headline":"[&quot;Learning Interactions and Relationships between Movie Characters&quot;]","description":"[&quot;CVPR 2020 (Oral)&quot;]","name":"[&quot;Learning Interactions and Relationships between Movie Characters&quot;]","@context":"https://schema.org"}</script>
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="theme-color" content="#157878">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <link rel="stylesheet" href="style.css" />
    <title>
      ["Leveraging Self-Supervised Training for Unintentional Action
      Recognition"] ["ECCVW 2022 (SSLWIN)"]
    </title>
  </head>
  <body class="container-fluid">
    <header class="page-header" role="banner">
      <h1 class="project-name">
        Leveraging Self-Supervised Training for Unintentional Action Recognition
      </h1>
      <h2 class="project-tagline">ECCVW 2022 (SSLWIN)</h2>
      <a href="https://arxiv.org/pdf/2209.11870.pdf" class="btn">Paper</a>
      <a class="btn">Code (Coming soon...)</a>
    </header>
    <main id="content" class="main-content" role="main">
      <div align="center">
        <p>
          <a href="https://www.linkedin.com/in/eneaduka/">Enea Duka</a
          ><sup>*</sup>, <a href="https://annusha.github.io">Anna Kukleva</a
          ><sup>*</sup>,
          <a
            href="https://www.mpi-inf.mpg.de/departments/computer-vision-and-machine-learning/people/bernt-schiele"
            >Bernt Schiele</a
          >
        </p>
      </div>
      <div align="center">
        <p>Max-Planck-Institute for Informatics, Saarbr√ºcken, Germany</p>
      </div>
      <hr />

      <div align="center">
        <img
          src="gifs/example.gif"
          width="480"
          height="270"
          class="giphy-embed"
        ></img>
      </div>
      <hr />
      <div align="center">
        <h3 id="abstract-div">Abstract</h3>
      </div>
      <p>
        Unintentional actions are rare occurrences that are difficult to define
        precisely and that are highly dependent on the temporal context of the
        action. In this work, we explore such actions and seek to identify the
        points in videos where the actions transition from intentional to
        unintentional. We propose a multi-stage framework that exploits inherent
        biases such as motion speed, motion direction, and order to recognize
        unintentional actions. To enhance representations via self-supervised
        training for the task of unintentional action recognition we propose
        temporal transformations, called <strong>T</strong>emporal
        <strong>T</strong>ransformations of <strong>I</strong>nherent
        <strong>B</strong>iases of <strong>U</strong>nintentional
        <strong>A</strong>ctions (T<sup>2</sup>IBUA). The multi-stage approach
        models the temporal information on both the level of individual frames
        and full clips. These enhanced representations show strong performance
        for unintentional action recognition tasks. We provide an extensive
        ablation study of our framework and report results that significantly
        improve over the state-of-the-art.
      </p>
      <hr />
      <div align="center">
        <h3 id="bibtex-div">BibTex</h3>
      </div>
      <pre><code>@inproceedings{duka2022leveraging,
        title={Leveraging Self-Supervised Training for Unintentional Action Recognition},
        author={Duka, Enea and Kukleva, Anna and Schiele, Bernt},
        booktitle={European Conference on Computer Vision Workshop SSLWIN (ECCVW)},
        year={2022},
        organization={Springer}
      }
  </code></pre>

      <footer class="site-footer">
        <!-- <span class="site-footer-owner"
          ><a href="https://github.com/Annusha/LIReC">LIReC</a> is maintained by
          <a href="https://github.com/Annusha">Annusha</a>.</span
        > -->

        <span class="site-footer-credits"
          >This page was generated by
          <a href="https://pages.github.com">GitHub Pages</a>.</span
        >
      </footer>
    </main>
  </body>
</html>
